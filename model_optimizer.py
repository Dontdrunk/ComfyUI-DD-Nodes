import torch
import folder_paths
import comfy.sd
import comfy.utils
import comfy.model_management as model_management
import sys
import gc
import psutil
from datetime import datetime
import time
from pathlib import Path

class DDModelOptimizer:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "æ¨¡å‹æ–‡ä»¶": (folder_paths.get_filename_list("diffusion_models"), ),
                "æ™ºèƒ½æ¨¡å¼": ("BOOLEAN", {"default": False}),
                "åŠ è½½æ¨¡å¼": ([
                    "æ ‡å‡†åŠ è½½",
                    "åˆ†æ­¥åŠ è½½",
                ],),
                "ä¼˜åŒ–æ¨¡å¼": ([
                    "ç¦ç”¨ä¼˜åŒ–",             # æ”¹ä¸ºæ›´ç›´è§‚çš„åç§°
                    "FP8åŸºç¡€å†…å­˜ä¼˜åŒ–",      # fp8_e4m3fn
                    "FP8é«˜é€Ÿæ€§èƒ½ä¼˜åŒ–",      # fp8_e4m3fn_fast
                    "FP8ç¨³å®šè´¨é‡ä¼˜åŒ–"       # fp8_e5m2
                ],),
            }
        }
    
    RETURN_TYPES = ("MODEL",)
    RETURN_NAMES = ("ä¼˜åŒ–æ¨¡å‹",)
    FUNCTION = "optimize_model"
    CATEGORY = "ğŸºDDç³»åˆ—èŠ‚ç‚¹"

    def get_system_info(self):
        """è·å–ç³»ç»Ÿé…ç½®ä¿¡æ¯"""
        system_info = {
            "total_memory": psutil.virtual_memory().total / (1024**3),  # GB
            "available_memory": psutil.virtual_memory().available / (1024**3),  # GB
            "cpu_count": psutil.cpu_count(logical=False),
            "gpu_info": None,
        }
        
        if torch.cuda.is_available():
            system_info["gpu_info"] = {
                "name": torch.cuda.get_device_name(),
                "total_memory": torch.cuda.get_device_properties(0).total_memory / (1024**3),  # GB
                "free_memory": torch.cuda.memory_allocated(0) / (1024**3),  # GB
            }
        return system_info

    def determine_smart_options(self, system_info, model_path):
        """æ ¹æ®ç³»ç»Ÿé…ç½®å’Œæ¨¡å‹æ™ºèƒ½ç¡®å®šæœ€ä½³é€‰é¡¹
        
        Args:
            system_info: ç³»ç»Ÿä¿¡æ¯
            model_path: æ¨¡å‹è·¯å¾„
        """
        try:
            # å°†å­—ç¬¦ä¸²è·¯å¾„è½¬æ¢ä¸ºPathå¯¹è±¡
            path_obj = Path(model_path)
            # å®‰å…¨è·å–æ–‡ä»¶å¤§å°
            if path_obj.exists():
                model_size = path_obj.stat().st_size / (1024**3)  # GB
                print(f"\næ¨¡å‹æ–‡ä»¶å¤§å°: {model_size:.2f}GB")
            else:
                print(f"è­¦å‘Š: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                model_size = 0
                
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è·å–æ¨¡å‹æ–‡ä»¶ä¿¡æ¯: {str(e)}")
            model_size = 0
        
        options = {
            "åŠ è½½æ¨¡å¼": "æ ‡å‡†åŠ è½½",
            "ä¼˜åŒ–æ¨¡å¼": "ç¦ç”¨ä¼˜åŒ–"
        }
        
        # è·å–æ˜¾å­˜å’Œå†…å­˜ä¿¡æ¯
        vram = 0
        ram = system_info["total_memory"]
        
        if system_info["gpu_info"]:
            vram = system_info["gpu_info"]["total_memory"]
            print(f"GPUæ€»æ˜¾å­˜: {vram:.2f}GB")
            print(f"ç³»ç»Ÿæ€»å†…å­˜: {ram:.2f}GB")
        
        # è®¾ç½®ç¼“å†²ç³»æ•°
        vram_scale_factor = 1.5
        ram_scale_factor = 1.5
        
        # å†³å®šåŠ è½½æ¨¡å¼
        # å¦‚æœæ¨¡å‹å¤§å°å¤§äºå¯ç”¨æ˜¾å­˜çš„70%æˆ–å¤§äºæ˜¾å­˜æ€»é‡ï¼Œä½¿ç”¨åˆ†æ­¥åŠ è½½
        if model_size > vram * 0.7:
            options["åŠ è½½æ¨¡å¼"] = "åˆ†æ­¥åŠ è½½"
            print("ç”±äºæ¨¡å‹å¤§å°è¶…è¿‡å¯ç”¨æ˜¾å­˜çš„70%ï¼Œé€‰æ‹©åˆ†æ­¥åŠ è½½æ¨¡å¼")
        
        # åº”ç”¨æ–°çš„ä¼˜åŒ–è§„åˆ™
        if model_size <= vram:
            options["ä¼˜åŒ–æ¨¡å¼"] = "ç¦ç”¨ä¼˜åŒ–"
            print("æ¨¡å‹å®Œå…¨é€‚é…æ˜¾å­˜ï¼Œè·å¾—æœ€ä½³è´¨é‡")
            
        elif model_size <= vram * vram_scale_factor and model_size <= ram:
            options["ä¼˜åŒ–æ¨¡å¼"] = "ç¦ç”¨ä¼˜åŒ–"
            print("æ¨¡å‹å¯é€šè¿‡æ˜¾å­˜+è™šæ‹Ÿå†…å­˜åŠ è½½ï¼Œä¿æŒæœ€ä½³è´¨é‡")
            
        elif model_size > vram * vram_scale_factor and model_size <= ram:
            options["ä¼˜åŒ–æ¨¡å¼"] = "FP8ç¨³å®šè´¨é‡ä¼˜åŒ–"
            print("æ¨¡å‹æ˜¾è‘—è¶…å‡ºæ˜¾å­˜ä½†é€‚é…å†…å­˜ï¼Œå¯åŠ¨è´¨é‡ä¼˜å…ˆä¼˜åŒ–")
            
        elif model_size > ram and model_size <= ram * ram_scale_factor:
            options["ä¼˜åŒ–æ¨¡å¼"] = "FP8åŸºç¡€å†…å­˜ä¼˜åŒ–"
            print("æ¨¡å‹è¶…å‡ºç‰©ç†å†…å­˜ï¼Œå¯åŠ¨å†…å­˜ä¼˜åŒ–")
            
        else:  # model_size > ram * ram_scale_factor
            options["ä¼˜åŒ–æ¨¡å¼"] = "FP8é«˜é€Ÿæ€§èƒ½ä¼˜åŒ–"
            print("æ¨¡å‹ä¸¥é‡è¶…å‡ºç‰©ç†å†…å­˜ï¼Œå¯åŠ¨æ€§èƒ½ä¼˜å…ˆä¼˜åŒ–")
        
        return options

    def show_step_progress(self, step_name, progress=0):
        """æ˜¾ç¤ºæ­¥éª¤è¿›åº¦
        Args:
            step_name: æ­¥éª¤åç§°
            progress: è¿›åº¦å€¼(0-100)
        """
        bar_width = 30
        filled = int(bar_width * progress / 100)
        bar = '#' * filled + '-' * (bar_width - filled)
        sys.stdout.write(f'\r{step_name} [{bar}] {progress}%')
        sys.stdout.flush()
        if progress >= 100:
            print()

    def optimize_model(self, æ¨¡å‹æ–‡ä»¶, æ™ºèƒ½æ¨¡å¼, åŠ è½½æ¨¡å¼, ä¼˜åŒ–æ¨¡å¼):
        """
        æ¨¡å‹ä¼˜åŒ–åŠ è½½ä¸»å‡½æ•°
        Args:
            æ¨¡å‹æ–‡ä»¶: è¦åŠ è½½çš„æ¨¡å‹æ–‡ä»¶å
            æ™ºèƒ½æ¨¡å¼: æ˜¯å¦å¯ç”¨æ™ºèƒ½æ¨¡å¼
            åŠ è½½æ¨¡å¼: æ ‡å‡†åŠ è½½æˆ–åˆ†æ­¥åŠ è½½
            ä¼˜åŒ–æ¨¡å¼: ç¦ç”¨ä¼˜åŒ–/FP8åŸºç¡€å†…å­˜ä¼˜åŒ–/FP8é«˜é€Ÿæ€§èƒ½ä¼˜åŒ–/FP8ç¨³å®šè´¨é‡ä¼˜åŒ–
        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹
        """
        start_time = time.time()
        print(f"\nå¼€å§‹å¤„ç†æ¨¡å‹: {æ¨¡å‹æ–‡ä»¶}")
        print(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # è·å–æ¨¡å‹è·¯å¾„
            model_path = folder_paths.get_full_path_or_raise("diffusion_models", æ¨¡å‹æ–‡ä»¶)

            # å¦‚æœå¯ç”¨æ™ºèƒ½æ¨¡å¼ï¼Œé‡æ–°ç¡®å®šåŠ è½½æ¨¡å¼å’Œä¼˜åŒ–æ¨¡å¼
            if æ™ºèƒ½æ¨¡å¼:
                system_info = self.get_system_info()
                smart_options = self.determine_smart_options(system_info, model_path)
                åŠ è½½æ¨¡å¼ = smart_options["åŠ è½½æ¨¡å¼"]
                ä¼˜åŒ–æ¨¡å¼ = smart_options["ä¼˜åŒ–æ¨¡å¼"]
                print("\næ™ºèƒ½æ¨¡å¼å·²å¯ç”¨:")
                print(f"ç³»ç»Ÿé…ç½®: {'GPU: ' + system_info['gpu_info']['name'] if system_info['gpu_info'] else 'CPU'}")
                print(f"è‡ªåŠ¨é€‰æ‹© - åŠ è½½æ¨¡å¼: {åŠ è½½æ¨¡å¼}")
                print(f"è‡ªåŠ¨é€‰æ‹© - ä¼˜åŒ–æ¨¡å¼: {ä¼˜åŒ–æ¨¡å¼}")

            # è®¾ç½®ä¼˜åŒ–é€‰é¡¹
            model_options = {}
            needs_optimization = True  # æ˜¯å¦éœ€è¦è¿›è¡Œä¼˜åŒ–å¤„ç†

            if ä¼˜åŒ–æ¨¡å¼ == "FP8åŸºç¡€å†…å­˜ä¼˜åŒ–":
                print("ä½¿ç”¨ FP8 åŸºç¡€å†…å­˜ä¼˜åŒ–æ¨¡å¼")
                model_options["dtype"] = torch.float8_e4m3fn
            elif ä¼˜åŒ–æ¨¡å¼ == "FP8é«˜é€Ÿæ€§èƒ½ä¼˜åŒ–":
                print("ä½¿ç”¨ FP8 é«˜é€Ÿæ€§èƒ½ä¼˜åŒ–æ¨¡å¼")
                model_options["dtype"] = torch.float8_e4m3fn
                model_options["fp8_optimizations"] = True
            elif ä¼˜åŒ–æ¨¡å¼ == "FP8ç¨³å®šè´¨é‡ä¼˜åŒ–":
                print("ä½¿ç”¨ FP8 ç¨³å®šè´¨é‡ä¼˜åŒ–æ¨¡å¼")
                model_options["dtype"] = torch.float8_e5m2
            else:
                print("ä¼˜åŒ–å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹åŠ è½½æ¨¡å¼")
                needs_optimization = False

            if åŠ è½½æ¨¡å¼ == "æ ‡å‡†åŠ è½½":
                print("ä½¿ç”¨æ ‡å‡†åŠ è½½æ¨¡å¼...")
                self.show_step_progress("åŠ è½½æ¨¡å‹", 0)
                model = comfy.sd.load_diffusion_model(model_path, model_options=model_options)
                self.show_step_progress("åŠ è½½æ¨¡å‹", 100)
                
            else:
                print("ä½¿ç”¨åˆ†æ­¥åŠ è½½æ¨¡å¼...")
                
                # ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ¨¡å‹æ–‡ä»¶
                print("æ­¥éª¤1: åŠ è½½æ¨¡å‹æ–‡ä»¶")
                self.show_step_progress("åŠ è½½æ–‡ä»¶", 0)
                state_dict = comfy.utils.load_torch_file(model_path)
                self.show_step_progress("åŠ è½½æ–‡ä»¶", 100)
                
                # ç¬¬äºŒæ­¥ï¼šé¢„å¤„ç†æƒé‡
                print("\næ­¥éª¤2: é¢„å¤„ç†æƒé‡")
                total_keys = len(state_dict)
                processed = 0
                update_interval = max(1, total_keys // 100)  # ç¡®ä¿è‡³å°‘æ˜¾ç¤º100ä¸ªæ›´æ–°ç‚¹

                if needs_optimization:  # åªåœ¨éœ€è¦ä¼˜åŒ–æ—¶è¿›è¡Œå¤„ç†
                    for key, value in state_dict.items():
                        if torch.is_tensor(value):
                            state_dict[key] = value.to(model_options["dtype"])
                        processed += 1
                        if processed % update_interval == 0:
                            progress = int(processed / total_keys * 100)
                            self.show_step_progress("å¤„ç†æƒé‡", progress)
                self.show_step_progress("å¤„ç†æƒé‡", 100)
                
                # ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ¨¡å‹
                print("\næ­¥éª¤3: æ„å»ºæ¨¡å‹")
                self.show_step_progress("æ„å»ºæ¨¡å‹", 0)
                model = comfy.sd.load_diffusion_model_state_dict(state_dict, model_options)
                self.show_step_progress("æ„å»ºæ¨¡å‹", 100)

            # æ¸…ç†å†…å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            end_time = time.time()
            duration = end_time - start_time
            print(f"\næ¨¡å‹åŠ è½½å®Œæˆï¼ç”¨æ—¶: {duration:.2f}ç§’")
            return (model,)
            
        except Exception as e:
            print(f"\næ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise e

NODE_CLASS_MAPPINGS = {
    "DD-ModelOptimizer": DDModelOptimizer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "DD-ModelOptimizer": "DD æ¨¡å‹ä¼˜åŒ–åŠ è½½"
}
