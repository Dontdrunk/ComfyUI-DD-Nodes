import torch
import numpy as np
import cv2
import comfy.utils
from typing import List, Dict, Any, Tuple

class DDImageUniformSize:
    """
    DD å›¾åƒç»Ÿä¸€å°ºå¯¸ - å°†è¾“å…¥çš„å›¾åƒæˆ–è§†é¢‘ç»Ÿä¸€è°ƒæ•´ä¸ºæŒ‡å®šåˆ†è¾¨ç‡
    æ”¯æŒå¤šè¾“å…¥ç«¯å£ï¼Œå¤šç§ç¼©æ”¾æ–¹æ³•å’Œå°ºå¯¸é€‚é…ç­–ç•¥
    åªæœ‰æ¥å…¥å†…å®¹çš„è¾“å…¥ç«¯å£æ‰ä¼šç”Ÿæˆå¯¹åº”çš„è¾“å‡º
    """

    @classmethod
    def INPUT_TYPES(cls):
        # åŸºæœ¬é…ç½®
        inputs = {
            "required": {
                "ç¼©æ”¾æ–¹æ³•": (["é‚»è¿‘-ç²¾ç¡®", "åŒçº¿æ€§æ’å€¼", "åŒºåŸŸ", "åŒä¸‰æ¬¡æ’å€¼", "lanczos"], {"default": "åŒçº¿æ€§æ’å€¼"}),
                "å®½åº¦": ("INT", {"default": 512, "min": 8, "max": 8192, "step": 8}),
                "é«˜åº¦": ("INT", {"default": 512, "min": 8, "max": 8192, "step": 8}),
                "å°ºå¯¸é€‚é…": (["è‡ªé€‚åº”", "æ‹‰ä¼¸", "è£å‰ª", "å¡«å……"], {"default": "è‡ªé€‚åº”"}),
            },
            "optional": {
                # å››ä¸ªå›ºå®šçš„å¯é€‰è¾“å…¥ç«¯å£
                "å›¾ç‰‡A": ("IMAGE",),
                "å›¾ç‰‡B": ("IMAGE",),
                "å›¾ç‰‡C": ("IMAGE",),
                "å›¾ç‰‡D": ("IMAGE",),
            }
        }
        return inputs

    # é»˜è®¤è¾“å‡ºç«¯å£è®¾ç½® - æ‰€æœ‰å¯èƒ½çš„è¾“å‡º
    RETURN_TYPES = ("IMAGE", "IMAGE", "IMAGE", "IMAGE")
    RETURN_NAMES = ("å›¾ç‰‡A", "å›¾ç‰‡B", "å›¾ç‰‡C", "å›¾ç‰‡D")
    FUNCTION = "resize_images"
    CATEGORY = "ğŸºDDç³»åˆ—èŠ‚ç‚¹"

    # å¸¸é‡ï¼šæ’å€¼æ–¹æ³•æ˜ å°„
    INTERPOLATION_MAP = {
        "é‚»è¿‘-ç²¾ç¡®": cv2.INTER_NEAREST_EXACT,
        "åŒçº¿æ€§æ’å€¼": cv2.INTER_LINEAR,
        "åŒºåŸŸ": cv2.INTER_AREA,
        "åŒä¸‰æ¬¡æ’å€¼": cv2.INTER_CUBIC,
        "lanczos": cv2.INTER_LANCZOS4
    }

    def _resize_batch(self, image_batch, target_size, interpolation_mode, size_adapt):
        """è°ƒæ•´ä¸€æ‰¹å›¾åƒçš„å¤§å°"""
        if image_batch is None:
            return None
            
        # ç¡®ä¿è¾“å…¥ä¸ºå¼ é‡
        if not isinstance(image_batch, torch.Tensor):
            return None
            
        # è·å–å›¾åƒå°ºå¯¸
        if len(image_batch.shape) == 3:  # å•å¼ å›¾ç‰‡ [H, W, C]
            image_batch = image_batch.unsqueeze(0)  # [1, H, W, C]
        
        batch_size, height, width, channels = image_batch.shape
        target_height, target_width = target_size
        result = None
        
        # è·å–æ’å€¼æ–¹æ³•
        interpolation = self.INTERPOLATION_MAP.get(interpolation_mode, cv2.INTER_LINEAR)
        
        # æ ¹æ®å°ºå¯¸é€‚é…æ–¹æ³•è°ƒæ•´å›¾åƒ
        if size_adapt == "æ‹‰ä¼¸":
            # ç›´æ¥è°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
            result = self._batch_resize(image_batch, target_size, interpolation)
        
        elif size_adapt == "è‡ªé€‚åº”":
            # ä¿æŒå®½é«˜æ¯”ç¼©æ”¾
            result = self._batch_adaptive_resize(image_batch, target_size, interpolation)
            
        elif size_adapt == "è£å‰ª":
            # ä¿æŒå®½é«˜æ¯”ç¼©æ”¾åå±…ä¸­è£å‰ª
            result = self._batch_center_crop(image_batch, target_size, interpolation)
            
        elif size_adapt == "å¡«å……":
            # ä¿æŒå®½é«˜æ¯”ç¼©æ”¾åå¡«å……
            result = self._batch_pad(image_batch, target_size, interpolation)
            
        else:
            # é»˜è®¤ä½¿ç”¨è‡ªé€‚åº”
            result = self._batch_adaptive_resize(image_batch, target_size, interpolation)
        
        return result

    def _batch_resize(self, batch, target_size, interpolation):
        """æ‰¹é‡è°ƒæ•´å°ºå¯¸ - ç›´æ¥æ‹‰ä¼¸"""
        # ä½¿ç”¨PyTorchå†…ç½®çš„resizeå‡½æ•°
        target_height, target_width = target_size
        # å°†å›¾åƒä»BHWCè½¬æ¢ä¸ºBCHW
        batch_bchw = batch.permute(0, 3, 1, 2) 
        
        resized = torch.nn.functional.interpolate(
            batch_bchw,
            size=(target_height, target_width),
            mode=self._get_torch_mode(interpolation),
            align_corners=False if self._get_torch_mode(interpolation) != 'nearest' else None
        )
        
        # è½¬å›BHWC
        return resized.permute(0, 2, 3, 1)

    def _get_torch_mode(self, cv2_interpolation):
        """å°†OpenCVæ’å€¼æ¨¡å¼è½¬æ¢ä¸ºPyTorchæ¨¡å¼"""
        if cv2_interpolation in [cv2.INTER_NEAREST, cv2.INTER_NEAREST_EXACT]:
            return 'nearest'
        elif cv2_interpolation == cv2.INTER_LINEAR:
            return 'bilinear'
        elif cv2_interpolation == cv2.INTER_CUBIC:
            return 'bicubic'
        elif cv2_interpolation == cv2.INTER_AREA:
            return 'area'
        else:
            return 'bilinear'  # é»˜è®¤è¿”å›åŒçº¿æ€§

    def _batch_adaptive_resize(self, batch, target_size, interpolation):
        """æ‰¹é‡è‡ªé€‚åº”è°ƒæ•´å°ºå¯¸ - ä¿æŒå®½é«˜æ¯”"""
        batch_size, height, width, channels = batch.shape
        target_height, target_width = target_size
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        ratio = min(target_width / width, target_height / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        
        # å…ˆè°ƒæ•´å¤§å°
        batch_bchw = batch.permute(0, 3, 1, 2)
        resized = torch.nn.functional.interpolate(
            batch_bchw,
            size=(new_height, new_width),
            mode=self._get_torch_mode(interpolation),
            align_corners=False if self._get_torch_mode(interpolation) != 'nearest' else None
        )
        
        # åˆ›å»ºç›®æ ‡å¤§å°çš„ç©ºå¼ é‡
        result = torch.zeros(batch_size, channels, target_height, target_width, device=batch.device)
        
        # è®¡ç®—åç§»é‡
        y_offset = (target_height - new_height) // 2
        x_offset = (target_width - new_width) // 2
        
        # å°†è°ƒæ•´åçš„å›¾åƒæ”¾åœ¨ä¸­å¿ƒ
        result[:, :, y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized
        
        # è½¬å›BHWC
        return result.permute(0, 2, 3, 1)

    def _batch_center_crop(self, batch, target_size, interpolation):
        """æ‰¹é‡ä¸­å¿ƒè£å‰ª - å…ˆè°ƒæ•´å¤§å°ç„¶åè£å‰ª"""
        batch_size, height, width, channels = batch.shape
        target_height, target_width = target_size
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ - ä»¥è¾ƒå¤§çš„æ¯”ä¾‹ä¸ºå‡†ï¼Œç¡®ä¿è£å‰ª
        ratio = max(target_width / width, target_height / height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        
        # è°ƒæ•´å¤§å°
        batch_bchw = batch.permute(0, 3, 1, 2)
        resized = torch.nn.functional.interpolate(
            batch_bchw,
            size=(new_height, new_width),
            mode=self._get_torch_mode(interpolation),
            align_corners=False if self._get_torch_mode(interpolation) != 'nearest' else None
        )
        
        # è®¡ç®—è£å‰ªåŒºåŸŸ
        y_start = (new_height - target_height) // 2
        x_start = (new_width - target_width) // 2
        
        # è£å‰ªä¸­å¿ƒåŒºåŸŸ
        cropped = resized[:, :, y_start:y_start + target_height, x_start:x_start + target_width]
        
        # è½¬å›BHWC
        return cropped.permute(0, 2, 3, 1)

    def _batch_pad(self, batch, target_size, interpolation):
        """æ‰¹é‡å¡«å…… - ä¿æŒå®½é«˜æ¯”å¹¶å¡«å……"""
        # è¿™ä¸è‡ªé€‚åº”è°ƒæ•´ç›¸åŒï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åˆ›å»ºäº†å…¨é›¶èƒŒæ™¯å¹¶å±…ä¸­æ”¾ç½®è°ƒæ•´åçš„å›¾åƒ
        return self._batch_adaptive_resize(batch, target_size, interpolation)

    def resize_images(self, ç¼©æ”¾æ–¹æ³•, å®½åº¦, é«˜åº¦, å°ºå¯¸é€‚é…, å›¾ç‰‡A=None, å›¾ç‰‡B=None, å›¾ç‰‡C=None, å›¾ç‰‡D=None):
        """æ ¹æ®æŒ‡å®šå‚æ•°ç»Ÿä¸€è°ƒæ•´æ‰€æœ‰è¾“å…¥å›¾åƒçš„å¤§å°"""
        target_size = (é«˜åº¦, å®½åº¦)  # (H, W)
        results = []
        
        # åˆ›å»ºè¾“å…¥å›¾åƒå’Œåç§°çš„æ˜ å°„
        input_images = {
            "å›¾ç‰‡A": å›¾ç‰‡A,
            "å›¾ç‰‡B": å›¾ç‰‡B,
            "å›¾ç‰‡C": å›¾ç‰‡C,
            "å›¾ç‰‡D": å›¾ç‰‡D
        }
        
        # è¿‡æ»¤å‡ºæœ‰å†…å®¹çš„è¾“å…¥
        valid_inputs = {name: img for name, img in input_images.items() if img is not None}
        
        # åŠ¨æ€è®¾ç½®è¾“å‡ºç±»å‹å’Œåç§°
        if len(valid_inputs) > 0:
            self.RETURN_TYPES = tuple(["IMAGE"] * len(valid_inputs))
            self.RETURN_NAMES = tuple(valid_inputs.keys())
        else:
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¾“å…¥ï¼Œæä¾›ä¸€ä¸ªé»˜è®¤è¾“å‡º
            self.RETURN_TYPES = ("IMAGE",)
            self.RETURN_NAMES = ("å›¾ç‰‡A",)
            # åˆ›å»ºé»˜è®¤ç©ºå›¾åƒ
            empty_image = torch.zeros(1, é«˜åº¦, å®½åº¦, 3)
            return (empty_image,)
        
        # å¤„ç†æ¯ä¸ªæœ‰æ•ˆè¾“å…¥
        for img in valid_inputs.values():
            # è°ƒæ•´å›¾åƒå¤§å°
            resized = self._resize_batch(img, target_size, ç¼©æ”¾æ–¹æ³•, å°ºå¯¸é€‚é…)
            results.append(resized)
            
        return tuple(results)

# èŠ‚ç‚¹ç±»æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "DD-ImageUniformSize": DDImageUniformSize
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "DD-ImageUniformSize": "DD å›¾åƒç»Ÿä¸€å°ºå¯¸"
}
