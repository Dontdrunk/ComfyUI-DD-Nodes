import torch
import comfy.model_management as model_management
import gc
import time

class DDSamplingOptimizer:
    """
    DD é‡‡æ ·é€Ÿåº¦ä¼˜åŒ–å™¨ - æç®€ç‰ˆ
    ä¼˜åŒ–æ¨¡å‹é¦–æ¬¡é‡‡æ ·é€Ÿåº¦ï¼Œå‡å°‘ç¬¬ä¸€æ¬¡é‡‡æ ·å»¶è¿Ÿ
    é€šè¿‡é¢„çƒ­æ¨¡å‹å’ŒCLIPå‚æ•°å‡å°‘é¦–æ¬¡é‡‡æ ·æ—¶çš„CUDAå»¶è¿Ÿ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æ¨¡å‹": ("MODEL",),
                "CLIPæ¨¡å‹": ("CLIP",)
            }
        }
    
    RETURN_TYPES = ("MODEL", "CLIP",)
    RETURN_NAMES = ("ä¼˜åŒ–æ¨¡å‹", "ä¼˜åŒ–CLIP",)
    FUNCTION = "optimize_sampling"
    CATEGORY = "ğŸºDDç³»åˆ—èŠ‚ç‚¹"

    def log_progress(self, message):
        """è¾“å‡ºè¿›åº¦ä¿¡æ¯"""
        print(f"[é‡‡æ ·ä¼˜åŒ–å™¨] {message}")
    
    def warmup_model(self, model):
        """æ¨¡å‹é¢„çƒ­æ ¸å¿ƒæ–¹æ³• - åŸºäºComfyUIå®˜æ–¹å®ç°çš„ä¼˜åŒ–æ–¹æ¡ˆ"""
        try:
            # å¯¹ModelPatcherç±»å‹çš„ç‰¹æ®Šå¤„ç†
            if hasattr(model, "model") and model.model is not None:
                self.log_progress("é€šè¿‡modelå±æ€§è®¿é—®åº•å±‚æ¨¡å‹")
                
                # è®¿é—®å†…éƒ¨æ¨¡å‹çš„æ¨¡å—å‚æ•°
                if hasattr(model.model, "modules"):
                    self.log_progress("è®¿é—®å†…éƒ¨æ¨¡å‹çš„modules")
                    for module in model.model.modules():
                        if hasattr(module, "_parameters"):
                            for param in module._parameters.values():
                                if param is not None:
                                    # åªæ˜¯è®¿é—®å‚æ•°ä»¥è§¦å‘CUDAé¢„çƒ­
                                    _ = param.device
                                    break
                
                # å°è¯•è®¿é—®modelå±æ€§çš„å‚æ•°
                self.log_progress("å°è¯•é€šè¿‡modelå±æ€§è®¿é—®å‚æ•°")
                try:
                    for param in model.model.parameters():
                        _ = param.device
                        break
                    self.log_progress("modelå‚æ•°è®¿é—®æˆåŠŸ")
                except Exception:
                    pass
            
            # å¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—®æ¨¡å‹
            elif hasattr(model, "modules"):
                self.log_progress("ç›´æ¥è®¿é—®æ¨¡å‹modules")
                for module in model.modules():
                    if hasattr(module, "_parameters"):
                        for param in module._parameters.values():
                            if param is not None:
                                _ = param.device
                                break
                
            self.log_progress("æ¨¡å‹åŸºæœ¬ä¼˜åŒ–å®Œæˆ")
            return True
        except Exception as e:
            self.log_progress(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
            return False
    
    def warmup_clip(self, clip_model):
        """CLIPæ¨¡å‹é¢„çƒ­æ ¸å¿ƒæ–¹æ³• - ä¿ç•™æœ€å¯é çš„æ–¹æ³•"""
        self.log_progress("CLIPé¢„çƒ­å¼€å§‹")
        try:
            with torch.no_grad():
                # å°è¯•æœ€å¸¸è§çš„CLIPæ–¹æ³•
                if hasattr(clip_model, "encode_from_tokens") and hasattr(clip_model, "tokenize"):
                    try:
                        empty_tokens = clip_model.tokenize("")
                        clip_model.encode_from_tokens(empty_tokens, return_pooled=True)
                        return
                    except Exception:
                        pass
                
                # å¤‡ç”¨æ–¹æ³•
                if hasattr(clip_model, "encode"):
                    try:
                        clip_model.encode("a photo")
                        return
                    except Exception:
                        pass
                
                # æœ€åŸºæœ¬çš„æ–¹æ³• - è®¿é—®å‚æ•°
                for param in clip_model.parameters():
                    _ = param.device
                    break
                
            self.log_progress("CLIPé¢„çƒ­å®Œæˆ")    
        except Exception as e:
            self.log_progress(f"CLIPé¢„çƒ­å¤±è´¥: {str(e)}")

    def optimize_sampling(self, æ¨¡å‹, CLIPæ¨¡å‹):
        """
        ä¼˜åŒ–æ¨¡å‹é¦–æ¬¡é‡‡æ ·é€Ÿåº¦ - ç®€åŒ–ç‰ˆ
        
        Args:
            æ¨¡å‹: éœ€è¦ä¼˜åŒ–çš„æ‰©æ•£æ¨¡å‹
            CLIPæ¨¡å‹: CLIPæ–‡æœ¬ç¼–ç å™¨æ¨¡å‹
            
        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹å’ŒCLIP
        """
        try:
            start_time = time.time()
            self.log_progress("å¼€å§‹ä¼˜åŒ–æ¨¡å‹é‡‡æ ·é€Ÿåº¦")
            
            # æ¸…ç†CUDAç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # æ‰§è¡Œæ¨¡å‹é¢„çƒ­
            self.log_progress("å¼€å§‹æ¨¡å‹é¢„çƒ­...")
            self.warmup_model(æ¨¡å‹)
            
            # CLIPæ¨¡å‹é¢„çƒ­
            self.log_progress("å¼€å§‹CLIPæ¨¡å‹é¢„çƒ­...")
            self.warmup_clip(CLIPæ¨¡å‹)
            
            # æ¸…ç†ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # ç»Ÿè®¡æ—¶é—´
            elapsed_time = time.time() - start_time
            self.log_progress(f"é‡‡æ ·ä¼˜åŒ–å®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’")
            self.log_progress("åç»­é‡‡æ ·è¿‡ç¨‹åº”è¯¥ä¸ä¼šå‡ºç°æ˜æ˜¾çš„é¦–æ¬¡å»¶è¿Ÿ")
            
            return (æ¨¡å‹, CLIPæ¨¡å‹)
            
        except Exception as e:
            self.log_progress(f"ä¼˜åŒ–è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            return (æ¨¡å‹, CLIPæ¨¡å‹)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "DD-SamplingOptimizer": DDSamplingOptimizer
}

# èŠ‚ç‚¹æ˜¾ç¤ºåç§°æ˜ å°„
NODE_DISPLAY_NAME_MAPPINGS = {
    "DD-SamplingOptimizer": "DD é‡‡æ ·ä¼˜åŒ–å™¨"
}
